This project addresses the need for efficient wildfire detection by implementing an unsupervised learning approach for segmenting fire regions in forest images. Utilizing a Convolutional Autoencoder (CAE), the model captures critical features from wildfire images without the need for labeled data. The CAE architecture consists of an encoder that compresses input images into a latent space representation and a decoder that reconstructs the images, forming the basis for segmentation. To enhance feature selection, Channel Attention blocks are incorporated within the CAE, enabling the model to prioritize essential fire-related features.

Clustering algorithms such as K-Means and Gaussian Mixture Model (GMM) are applied to the latent space to distinguish fire regions effectively. The Cluster Mask Enhancer module leverages these clustering outputs to create binary masks, which are then used to filter the red channel, highlighting fire-prone areas. Additionally, color-aware regularization is employed to ensure the model focuses on red hues typically associated with fire.

The model is trained and evaluated on the Corsican Fire Dataset, a collection of high-resolution wildfire images. Data augmentation techniques expand the dataset, enhancing the modelâ€™s robustness. Performance is assessed using metrics like Intersection over Union (IoU) and Dice Similarity Coefficient (DICE), with results demonstrating that this approach outperforms state-of-the-art models, achieving an IoU of 0.8398 and a DICE score of 0.8998.

For future work, the project aims to develop a video-based segmentation model to enable real-time wildfire detection across continuous frames. This enhancement would support timely intervention by identifying fire progression, potentially reducing wildfire impact on ecosystems and communities.
